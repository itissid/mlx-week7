{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966187f2b744\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/mlx-week7\n",
      "/workspace/mlx-week7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "# home = os.path.expanduser(\"~\")\n",
    "# home_path = Path(home)\n",
    "# print(home_path)\n",
    "project_path = Path(\"/workspace/mlx-week7\")\n",
    "print(project_path)\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/mlx-week7\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(project_path / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlx-week7/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModel,\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "assert os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    hf_token=os.environ[\"HF_TOKEN\"],\n",
    "    cache_dir=project_path / \"cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map  # NOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization routines for training and test data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 2048\n",
    "\n",
    "INST_START_TOKEN = \"[INST]\"\n",
    "INST_END_TOKEN = \"[/INST]\"\n",
    "NAV_START_TAG = \"[NAV]\"\n",
    "NAV_END_TAG = \"[/NAV]\"\n",
    "NL = \"\\n\"\n",
    "LANGUAGE = \"English\"\n",
    "REASON_START_TAG = \"[REASON]\"\n",
    "REASON_END_TAG = \"[/REASON]\"\n",
    "NUM_INSTRUCTIONS = 9\n",
    "BOS_TOKEN = \"<s>\"\n",
    "assert tokenizer.bos_token == BOS_TOKEN\n",
    "EOS_TOKEN = \"</s>\"\n",
    "assert tokenizer.eos_token == EOS_TOKEN\n",
    "\n",
    "\"\"\"\n",
    "There is a lot of important config here that fix_tokenizer will set up. IN the end it \n",
    "will be like:\n",
    "\n",
    "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', \n",
    "    vocab_size=32000, \n",
    "    model_max_length=1000000000000000019884624838656,\n",
    "    is_fast=True, \n",
    "    padding_side='left', \n",
    "    truncation_side='right', \n",
    "    special_tokens={\n",
    "        'bos_token': '<s>', \n",
    "        'eos_token': '</s>', \n",
    "        'unk_token': '<unk>', \n",
    "        'pad_token': '[PAD]', \n",
    "        'additional_special_tokens': ['[INST]', '[/INST]', '[NAV]', '[/NAV]', '[REASON]', '[/REASON]']\n",
    "    }, \n",
    "    clean_up_tokenization_spaces=False),  \n",
    "    added_tokens_decoder={\n",
    "            0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32000: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32001: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32000: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32001: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32002: AddedToken(\"[NAV]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32003: AddedToken(\"[/NAV]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32004: AddedToken(\"[REASON]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "            32005: AddedToken(\"[/REASON]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fix_tokenizer(tokenizer):\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    # https://huggingface.co/docs/transformers/en/pad_truncation#padding-and-truncation\n",
    "    # NOTE: special tokens added by default, padding si\n",
    "    tokenize = lambda prompt: tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "        # THIS IS IMPORTANT for calling the base tokenizer's build_inputs_with_special_tokens as\n",
    "        # explained https://huggingface.co/docs/transformers/v4.40.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n",
    "        # We already add it in the prompt preparation so we don't want to add it twice.\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    special_tokens_dict = {\n",
    "        \"additional_special_tokens\": [\n",
    "            INST_START_TOKEN,\n",
    "            INST_END_TOKEN,\n",
    "            NAV_START_TAG,\n",
    "            NAV_END_TAG,\n",
    "            REASON_START_TAG,\n",
    "            REASON_END_TAG,\n",
    "        ]\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "def generate_train_prompt(user_query):\n",
    "    sys_msg = \"Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using {NAV_START_TAG} and {NAV_END_TAG} tags as well as the reason tags:  {REASON_START_TAG} and {REASON_END_TAG}:as shown in the examples that follow:\"\n",
    "    p = (\n",
    "        f\"{BOS_TOKEN} {INST_START_TOKEN} \"\n",
    "        + sys_msg\n",
    "        + \"\\n\"\n",
    "        + user_query[\"chunk\"].strip()\n",
    "        + f\" {INST_END_TOKEN} \"\n",
    "        + user_query[\"navs\"].strip()\n",
    "        + f\" {EOS_TOKEN}\"\n",
    "    )\n",
    "    return p\n",
    "\n",
    "\n",
    "def generate_test_prompt(user_query):\n",
    "    sys_msg = f\"Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using {NAV_START_TAG} and {NAV_END_TAG} tags as well as the reason tags: {REASON_START_TAG} and {REASON_END_TAG}:\"\n",
    "    p = (\n",
    "        f\"{BOS_TOKEN} {INST_START_TOKEN} \"\n",
    "        + sys_msg\n",
    "        + \"\\n\"\n",
    "        + user_query[\"chunk\"].strip()\n",
    "        + f\" {INST_END_TOKEN} \"\n",
    "    )\n",
    "    return p\n",
    "\n",
    "\n",
    "tokenize = fix_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets make sure we can encode a test sentence and decode it back the way we want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using {NAV_START_TAG} and {NAV_END_TAG} tags as well as the reason tags:  {REASON_START_TAG} and {REASON_END_TAG}:as shown in the examples that follow:\n",
      "Test me [/INST] [NAV] test nav [/NAV][REASON] some reason [/REASON] </s>\n",
      "# tokens with padded ids 2048\n",
      "tensor([    1,   259, 32000, 28705, 12628,   272,  2245,   477,   264,  9863,\n",
      "          288,  3884,  1820, 18063,   264,  2948,  7103,  1059,   272,  2990,\n",
      "          302,  4222, 28725,  9131,  5099,   302,   272,  2245,   369,  6685,\n",
      "         2948, 18132, 11382,   297,   378,  1413,   371,  3384, 28790, 28730,\n",
      "        12241, 28730, 12137, 28752,   304,   371,  3384, 28790, 28730,  5000,\n",
      "        28730, 12137, 28752, 12944,   390,  1162,   390,   272,  2611, 12944,\n",
      "        28747, 28705,   371,   896,  2109,   832, 28730, 12241, 28730, 12137,\n",
      "        28752,   304,   371,   896,  2109,   832, 28730,  5000, 28730, 12137,\n",
      "        11290,   293,  4894,   297,   272,  9254,   369,  1372, 28747,    13,\n",
      "         1963,   528, 28705, 32001,   259, 32002, 28705,  1369,  5697, 28705,\n",
      "        32003, 32004, 28705,   741,  2611, 28705, 32005,   259,     2])\n",
      ".\n",
      "<s>  [INST]  Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using {NAV_START_TAG} and {NAV_END_TAG} tags as well as the reason tags:  {REASON_START_TAG} and {REASON_END_TAG}:as shown in the examples that follow:\n",
      "Test me [/INST]  [NAV]  test nav [/NAV][REASON]  some reason [/REASON]  </s>\n"
     ]
    }
   ],
   "source": [
    "test_prompt = generate_train_prompt(\n",
    "    {\n",
    "        \"chunk\": \"Test me\",\n",
    "        \"navs\": f\"{NAV_START_TAG} test nav {NAV_END_TAG}{REASON_START_TAG} some reason {REASON_END_TAG}\",\n",
    "    }\n",
    ")\n",
    "print(test_prompt)\n",
    "test_tokens = tokenize(test_prompt)\n",
    "print(f\"# tokens with padded ids {len(test_tokens['input_ids'])}\")\n",
    "input_ids = torch.tensor(test_tokens[\"input_ids\"])\n",
    "attn_mask = torch.tensor(test_tokens[\"attention_mask\"])\n",
    "\n",
    "unmasked_text = torch.masked_select(input_ids, attn_mask.bool())\n",
    "print(unmasked_text)\n",
    "# Now lets decode the text\n",
    "print(\".\")\n",
    "print(tokenizer.decode(unmasked_text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using [NAV] and [/NAV] tags as well as the reason tags: [REASON] and [/REASON]:\n",
      "Test me [/INST] \n",
      "# tokens with padded ids 2048\n",
      "tensor([    1,   259, 32000, 28705, 12628,   272,  2245,   477,   264,  9863,\n",
      "          288,  3884,  1820, 18063,   264,  2948,  7103,  1059,   272,  2990,\n",
      "          302,  4222, 28725,  9131,  5099,   302,   272,  2245,   369,  6685,\n",
      "         2948, 18132, 11382,   297,   378,  1413, 28705, 32002, 28705,   304,\n",
      "        28705, 32003, 28705, 12944,   390,  1162,   390,   272,  2611, 12944,\n",
      "        28747, 28705, 32004, 28705,   304, 28705, 32005,   714,    13,  1963,\n",
      "          528, 28705, 32001,   259])\n",
      ".\n",
      "<s>  [INST]  Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using [NAV]  and [/NAV]  tags as well as the reason tags: [REASON]  and [/REASON] :\n",
      "Test me [/INST]  \n"
     ]
    }
   ],
   "source": [
    "# Now for test tokenization\n",
    "test_prompt = generate_test_prompt(\n",
    "    {\n",
    "        \"chunk\": \"Test me\",\n",
    "    }\n",
    ")\n",
    "print(test_prompt)\n",
    "test_tokens = tokenize(test_prompt)\n",
    "print(f\"# tokens with padded ids {len(test_tokens['input_ids'])}\")\n",
    "input_ids = torch.tensor(test_tokens[\"input_ids\"])\n",
    "attn_mask = torch.tensor(test_tokens[\"attention_mask\"])\n",
    "\n",
    "unmasked_text = torch.masked_select(input_ids, attn_mask.bool())\n",
    "print(unmasked_text)\n",
    "# Now lets decode the text\n",
    "print(\".\")\n",
    "print(tokenizer.decode(unmasked_text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset for walking tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'chunk': 'Our route starts at the south side of Victoria Park, at Bonner Hall Bridge – best accessed by getting the Tube to Bethnal Green, a ten-minute walk away. The first portion of this route offers clues about the nature of east London before the arrival of the Regent’s Canal, taking in several buildings that predate the waterways, and whose fates were inexorably changed by its arrival. Before moving on, take a moment to reflect on Victoria Park. In the eighteenth century, this was all open pasture, interspersed with the odd brick kiln and market garden. The one notable feature was Bonner Hall, so called after the sixteenth-century bishop of London Edmund Bonner. All this was to change in the nineteenth century. As London expanded, calls for public parks grew; in 1840, Queen Victoria was presented with a petition signed by 30,000 residents. The Crown estate purchased 218 acres in the area and, over the next few years, converted it into Victoria Park. The park shares a family resemblance to Regent’s Park at the other end of the Regent’s Canal – no doubt down to the fact that its designer, James Pennethorne, was a protégé of none other than John Nash.6 The view south-east from Victoria Park’s Bonner Gate. When I first knew the park – many years ago – it was a fairly rough place. In particular the Pavilion Café, a few hundred metres to the east of Bonner Hall Bridge, was a grisly affair, best avoided unless one wanted to experience the stark reality of East End cuisine. Now, like many in east London, it has transformed into one of the best cafés mentioned in this book. It makes a splendid place to begin the walk if you’re in need of a bite to eat or drink. For those ready to plunge into Georgian industrial architecture, however, it is best to walk to the western end of the park and through the canal gate on to the towpath. Here, one can begin to imagine the feel of the canal in its early days: the canal side adjoining Victoria Park is used as moorings for barges, mostly residential, while across the waterway (which, as is usual, has a towpath – intended for the horses required to tow barges – generally on only one side) are',\n",
       "  'navs': '[NAV] Our route starts at the south side of Victoria Park, at Bonner Hall Bridge – best accessed by getting the Tube to Bethnal Green, a ten-minute walk away. [/NAV] [REASON] This was annotated because it provides specific instructions on where to start the route and how to get there, implying a direction to walk. [/REASON] [NAV] For those ready to plunge into Georgian industrial architecture, however, it is best to walk to the western end of the park and through the canal gate on to the towpath. [/NAV] [REASON] This was annotated because it provides specific instructions on where to walk next, indicating a direction to follow. [/REASON] '},\n",
       " {'chunk': 'This walk starts at one of the most famous landmarks in Britain: Tower Bridge. This bridge is a rare thing – a much-admired structure of great character, even though it was forged through endless committee meetings, compromise, contradiction, uncertainty and a fair degree of absurdity. It has a story worth considering at some length as you ascend from Tower Hill tube and stroll south across the bridge. The tale starts in the mid 1870s, when it was argued that a river crossing to the east of London Bridge would both improve communications in the City and, by providing direct access to industry on the south bank, increase the commercial potential of the docks on the north. There was, for practical reasons, only one site possible – a strip of land just east of the Tower of London – and a competition to find a suitable design was held. This was supervised by the City of London Corporation, which grabbed the initiative to build the new bridge. There was one key stipulation. It was not to block the access of tall-masted cargo ships to the wharves at the Pool of London, immediately to the west of the tower. To protect navigation the span of the proposed bridge had to be either incredibly high or it would have to open in some way to allow ships to pass through. Over fifty detailed designs were submitted. Most were weird and wonderful, including a swing bridge, a ‘roller’ bridge incorporating steam-operated sections that rolled from pier to pier, and a high-level bridge. All offered more or less workable solutions – some more expensive and complex than others – but all would visually overpower the historic tower and its setting. Many thought this far from desirable. Debate raged and two distinct interest groups emerged, each of which believed it should be responsible for the design and construction of the new bridge. There was the City Corporation, which had organised the competition and believed the proposed bridge was its responsibility. The City did, after all, begin just to the west of the site, and it had the funds to see the works to completion. They were to come from the City’s Bridge House estate, a body established in the early',\n",
       "  'navs': ' [NAV] This walk starts at one of the most famous landmarks in Britain: Tower Bridge. [/NAV] [REASON] This was annotated because it gives an implicit direction to start the walk at Tower Bridge. [/REASON] [NAV] ascend from Tower Hill tube and stroll south across the bridge. [/NAV] [REASON] This was annotated because it gives explicit directions to ascend from Tower Hill tube and stroll south across the bridge. [/REASON] [NAV] There was, for practical reasons, only one site possible – a strip of land just east of the Tower of London – [/NAV] [REASON] This was annotated because it gives an implicit direction to a specific location east of the Tower of London. [/REASON]'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "serialized_data_gpt_four = json.load(open(\"gpt_four_annotations.json\"))\n",
    "serialized_data_gpt_four_test = json.load(open(\"gpt_four_annotations_test.json\"))\n",
    "serialized_data_gpt_four[0], serialized_data_gpt_four_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(\n",
    "    [\n",
    "        len(s[\"chunk\"].strip().split()) + len(s[\"navs\"].strip().split())\n",
    "        for s in serialized_data_gpt_four\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model\n",
    "[ ] Train for a bit on this dataset\n",
    "[ ] Save the model.\n",
    "[ ] Test on some canned held out data from bermondsey street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del trainer\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = project_path / \"mixtral-moe-lora-instruct-walking-tour-london\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this flag to reload a pretrained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5c40897c784d53bf2bd54e20cd82ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f9a2afe6674f6bbdf817fbcb64ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=str(project_path / \"cache\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize the model based on the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32006, 4096)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively load the PEFT model from adapter\n",
    "See: https://huggingface.co/docs/transformers/en/peft#load-a-peft-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if load_pretrained:\n",
    "    # This can't work here. Cause the model has a changed shape due to fine tuning we did. \n",
    "    # To acually load it like this we would have to save our additional model on github and then \n",
    "    # use it\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_save_dir / \"best_saved_model\",\n",
    "    #     load_in_4bit=True,\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     device_map=\"auto\",\n",
    "    #     cache_dir=project_path / \"cache\",\n",
    "    # )\n",
    "\n",
    "    # But Even this is not quite right, we need to load a PEFT model using the LoraConfig\n",
    "    # and add the saved adapter\n",
    "    # model.load_adapter(model_save_dir / \"best_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32006, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32006, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "walking_tour_dataset = Dataset.from_list(serialized_data_gpt_four)\n",
    "walking_tour_dataset_test = Dataset.from_list(serialized_data_gpt_four_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c071e2b6ad04c499ead7153a5000aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9f30cd472648f789c2b23270afb379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_tour_dataset_tokens = walking_tour_dataset.shuffle().map(\n",
    "    lambda x: tokenize(generate_train_prompt(x)), remove_columns=[\"chunk\", \"navs\"]\n",
    ")\n",
    "walking_tour_dataset_tokens_test = walking_tour_dataset_test.shuffle().map(\n",
    "    lambda x: tokenize(generate_test_prompt(x)), remove_columns=[\"chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(walking_tour_dataset_tokens[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inject_adapter_in_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Hmm a shame the in place thing did not work. The issue is that thought it modifies the model \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# it does not change its type. \u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# inject_adapter_in_model(config, model, str(model_save_dir / \"best_saved_model\"))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_peft_model(model, config)\n\u001b[0;32m---> 23\u001b[0m     inject_adapter_in_model(config, model, \u001b[38;5;28mstr\u001b[39m(model_save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_saved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inject_adapter_in_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training\n",
    "from peft import inject_adapter_in_model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"w1\", \"w2\", \"w3\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "if not load_pretrained:\n",
    "    model = get_peft_model(model, config)\n",
    "else:\n",
    "    # Hmm a shame the in place thing did not work. The issue is that thought it modifies the model \n",
    "    # it does not change its type. \n",
    "    # inject_adapter_in_model(config, model, str(model_save_dir / \"best_saved_model\"))\n",
    "    model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MixtralForCausalLM(\n",
       "      (model): MixtralModel(\n",
       "        (embed_tokens): Embedding(32006, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MixtralDecoderLayer(\n",
       "            (self_attn): MixtralSdpaAttention(\n",
       "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MixtralRotaryEmbedding()\n",
       "            )\n",
       "            (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "              (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
       "              (experts): ModuleList(\n",
       "                (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "                  (w1): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (w2): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (w3): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                      (/workspace/mlx-week7/mixtral-moe-lora-instruct-walking-tour-london/checkpoint-74): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): MixtralRMSNorm()\n",
       "            (post_attention_layernorm): MixtralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MixtralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32006, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import inject_adapter_in_model\n",
    "inject_adapter_in_model(config, model, str(model_save_dir / \"checkpoint-74\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=walking_tour_dataset_tokens,\n",
    "    eval_dataset=walking_tour_dataset_tokens_test,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "\n",
    "        resume_from_checkpoint=str(model_save_dir),\n",
    "\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=2,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_total_limit=5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        overwrite_output_dir=True,\n",
    "        output_dir=str(model_save_dir),\n",
    "        load_best_model_at_end=True,\n",
    "        # resume_from_checkpoint=str(model_save_dir)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112/160 39:47 < 53:03, 0.02 it/s, Epoch 13.46/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.228300</td>\n",
       "      <td>2.391732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>2.380301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>2.452753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>2.572316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e26cf03c99f4cf2812e15b55f064afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(model_save_dir\u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-74\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/transformers/trainer.py:2249\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2245\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2246\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2247\u001b[0m     )\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2249\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2250\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m   2251\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2255\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2257\u001b[0m ):\n\u001b[1;32m   2258\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/accelerate/accelerator.py:2158\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2156\u001b[0m             acc_opt\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[0;32m-> 2158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:61\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m             norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads])\n\u001b[0;32m---> 61\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(torch\u001b[38;5;241m.\u001b[39mstack([norm\u001b[38;5;241m.\u001b[39mto(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-week7/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m             norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads])\n\u001b[0;32m---> 61\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(torch\u001b[38;5;241m.\u001b[39mstack([norm\u001b[38;5;241m.\u001b[39mto(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=str(model_save_dir/ \"checkpoint-74\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a pretrained model\n",
    "See https://huggingface.co/docs/transformers/v4.40.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_save_dir / \"best_saved_model\")\n",
    "# model.save_model(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a model from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    resume_from_checkpoint=\"mixtral-moe-lora-instruct-walking-tour-london/checkpoint-35\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"mixtral-moe-lora-instruct-walking-tour-london\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_dataset_chunks = json.load(open(\"./test_dataset.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_list(test_dataset_chunks)\n",
    "test_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt_gen = lambda text: \"<s> [INST]\" + sys_msg +\"\\n\"+ user_query[\"chunk\"] + \"[/INST]\" +  user_query[\"navs\"] + \"</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = generate_test_prompt(test_dataset_chunks[0])\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different tokenization strategies for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenize using encode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(test_prompt, add_special_tokens=False)\n",
    "print(encoded[0:100])\n",
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"<s>  [INST]  Given the text from a Walking tour book describing a specific route through the city of London, extract parts of the text that describe specific navigation instructions in it using  <NAV>  and </NAV>  tags as well as the reason tags: <REASON>  </REASON>  as shown in the examples that follow\n",
    "This walk starts at one of the most famous landmarks in Britain: Tower\n",
    "Bridge.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenize(test_prompt)\n",
    "print(len(test_tokens[\"input_ids\"]))\n",
    "test_input_ids = torch.tensor(test_tokens[\"input_ids\"])\n",
    "test_attn_mask = torch.tensor(test_tokens[\"attention_mask\"])\n",
    "print(len(test_input_ids), len(test_attn_mask))\n",
    "unmasked_text = torch.masked_select(test_input_ids, test_attn_mask.bool())\n",
    "print(unmasked_text[0:100])\n",
    "# Now lets decode the text\n",
    "print(\".\")\n",
    "print(tokenizer.decode(unmasked_text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer(\n",
    "    [test_prompt],\n",
    "    padding=\"max_length\",\n",
    "    max_length=200,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_text(text, max_length=50):\n",
    "    # Start with initial encoded text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # Generate text\n",
    "    for _ in tqdm(range(max_length)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        # Append the newly generated token ID to the existing input_ids tensor\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        # Print the updated text at each step\n",
    "        # print(\"Updated text:\", tokenizer.decode(input_ids.squeeze()))\n",
    "        # print(tokenizer.decode(next_token_id))\n",
    "        # # Optional: Stop on specific conditions, e.g., end of sentence\n",
    "        # 1f next_token_id in tokenizer.encode(l'.', '?'\n",
    "\n",
    "    # Return the final generated text\n",
    "    return tokenizer.decode(input_ids.squeeze(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset_chunks[0][\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = generate_text(test_dataset_chunks[0][\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_ids = output.logits[:, -1, :]\n",
    "torch.argmax(next_token_ids, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_no_padding = tokenizer(\n",
    "    test_prompt + f\"{NAV_START_TAG}\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "input_with_no_padding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_with_no_padding[\"input_ids\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "for _ in tqdm(range(200)):\n",
    "    with torch.no_grad():\n",
    "        input_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        token = tokenizer.decode(input_ids[:, -1].item())\n",
    "        if token == NAV_END_TAG:\n",
    "            input_ids = torch.cat(\n",
    "                (\n",
    "                    input_ids,\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            tokenizer.convert_tokens_to_ids(REASON_START_TAG),\n",
    "                        ]\n",
    "                    ).unsqueeze(0),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "        elif token == REASON_END_TAG:\n",
    "            input_ids = torch.cat(\n",
    "                (\n",
    "                    input_ids,\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            tokenizer.convert_tokens_to_ids(NAV_START_TAG),\n",
    "                        ]\n",
    "                    ).unsqueeze(0),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "        elif token == EOS_TOKEN:\n",
    "            break\n",
    "        # next_token_logits = outputs.logits[:, -1, :]\n",
    "        # next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "    # Append the newly generated token ID to the existing input_ids tensor\n",
    "\n",
    "    # input_ids = torch.cat([input_ids, outputs], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do what daniel taught me\n",
    "print(tokenizer.decode(input_ids.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([tokenizer.convert_tokens_to_ids(NAV_END_TAG)]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(\n",
    "    (\n",
    "        input_ids,\n",
    "        torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(NAV_END_TAG),\n",
    "                tokenizer.convert_tokens_to_ids(REASON_START_TAG),\n",
    "            ]\n",
    "        ).unsqueeze(0),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-1, -200, -1):\n",
    "    if tokenizer.decode(input_ids[:, i].item()) == NAV_END_TAG:\n",
    "        print(\"..\")\n",
    "    print(tokenizer.decode(input_ids[:, i].item()), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
